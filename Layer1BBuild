#####################
# ~ Run Source packages ~ #
#####################

#Run the functions for scripts
source('H:/NBA/Scripts/Package_Import.R')
source('H:/NBA/Scripts/Statistical_Functions.R')
source('H:/NBA/Scripts/Model_Functions.R')

#################################
# ~ Read in the pre processed dataset ~ #
#################################

username <- 'thatsmrlongcut'
password <- 'football17'

con <-
  mongo(
    collection = "NFL",
    db = "NBA_PRE_PROCESS",
    url = paste0("mongodb+srv://", username, ":", password, '@nfl-bsxce.mongodb.net/test?retryWrites=true&w=majority')
  )

df <- con$find()

#################################
# ~ Define the target variables now ~ #
#################################

variables <- list(
  TARGET = c('Spread', 'Over_Under', 'Total_Points', 'Points', 'Points_Alwd', 'Over_Under_Coverage', 'Over_Under_Differential',
             'Point_Differential', 'Spread_Differential', 'Spread_Coverage', 'Win_Loss'),
  ID = c('Set', 'Date', 'Team', 'Opponent'),
  
  COMMON = c('Home_Away', 'Season_Days', 'Month')
)

#################################
# ~ Merge with their opponents information ~ #
#################################

df <- 
  df %>%
  left_join(df %>%
              mutate(Team = Opponent) %>%
              dplyr::select(one_of(c('Date', 'Team', colnames(df)[!(colnames(df) %in% (do.call("c", variables) %>% as.character))]))),
            by = c('Date', 'Team'), suffix = c('', '_OPP')) %>%
  compare_cols2(., '_OPP') %>%
  mutate(Home_Away = ifelse(Home_Away == 'Home', 1, 0))

df <- df %>%
  mutate(id = row_number())

#Now assign 'A' or 'B' depending on what case it is
splits <- readRDS('H:/NBA/Model Build/Layer 1/a/splits.rds')

#Remove the rows that have already been trained on
df <-
  df %>%
  filter(id %in% splits$PARENT_SPLIT$df_id == F,
         Set == 'Layer 1') %>%
  na.omit()

#################################
# ~ Extract all layer 1.a predictions ~ #
#################################

#List the variables
y_vars <- variables$TARGET
#Positive Classes (for classification)
pos_classes <- c(NA, NA, NA, NA, NA, 'over', NA, NA, NA, 'covered', 'win')
path <- 'H:/NBA/Model Build/Layer 1/a/'

#Set working directory
setwd(path)

pred_df <-
  do.call("cbind.data.frame", 
  plyr::compact(lapply(1:length(y_vars), function(i) {
    #Declare the variable
    variable <- y_vars[i]
    
    if(paste0(variable, '.rds') %in% list.files()) {
      #Read in the bin of all models for that variable
      model_bin <- readRDS(paste0(variable, '.rds'))
      
      
      idf <-
        do.call("cbind.data.frame", lapply(1:length(model_bin), function(x) {
          model <- model_bin[[x]]$Model
          
          #Grab the features
          features <- model$features
          
          #Grab the type
          type <- model$task.desc$type
          
          #Make a copy of the data
          df_copy <- df %>%
            #Grab only the required features
            dplyr::select(one_of(c(variable, model$features)))
          
          #Rename the first column
          colnames(df_copy)[1] <- 'target'
          
          #If it is a classification task, identify the positive variable
          p_var <- pos_classes[i]
          
          #Transform into a task
          df_task <- if(type == 'classif') {
            makeClassifTask(data = df_copy,
                            target = 'target')
          } else {
            makeRegrTask(data = df_copy,
                         target = 'target')
          }
          
          #Make predictions from the model
          preds <-
            if(type == 'regr') {
              predict(model, df_task)$data$response } else {
                predict(model, df_task)$data[, paste0('prob.', p_var)]
              }
          
          #Make into a data frame
          preds <- data.frame(preds)
          
          #Rename column
          colnames(preds) <- paste0(variable, '_', x)
          
          #Return the data
          preds
        }))
    }
    
  })))

#Merge back with the variables
df <- cbind.data.frame(df %>%
                         dplyr::select(one_of(y_vars)),
                       pred_df)

#################################
# Construct the Splits #
#################################

#Split layer 1 into train/test .. this dataset will be used by all variables
trainRowNumbers <- createDataPartition(df[,'Point_Differential'], p = 0.7, list = FALSE)

sets <- list(
  Train = df[trainRowNumbers,],
  Test = df[-trainRowNumbers,]
)

#Establish the path to drop the Layer 1 a Models
path <- 'H:/NBA/Model Build/Layer 1/b/'

setwd(path)

#################################
# Construct Target Variables #
#################################

#Target variables
y_vars <- variables$TARGET

#Positive Classes (for classification)
pos_classes <- c(NA, NA, NA, NA, NA, 'over', NA, NA, NA, 'covered', 'win')

for(k in 1:length(y_vars)) {
  #Assign the target variable
  target_variable <- y_vars[k]
  
  #Make a copy of the sets
  data_copy <- sets
  
  #Rename the target variable in the copied data
  for(i in 1:length(data_copy)) {
    colnames(data_copy[[i]])[match(target_variable, colnames(data_copy[[i]]))] <- 'target'
  }
  
  #Make the target variable name consistent
  y <- 'target'
  
  #Declare the variables to be dropped
  drop_vars <- do.call("c", variables[1:2]) %>% as.character
  
  #Declare the identity column
  id <- 'id'
  
  #Define the X variables
  X <- colnames(data_copy$Train)[!(colnames(data_copy$Train) %in% c(y, id, drop_vars))]
  
  #Reduce the sets to just these variables
  data_copy <- lapply(1:length(data_copy), function(i) {
    data_copy[[i]] %>%
      dplyr::select(one_of(c(id, y, X)))
  })
  
  #If the target variable is numeric or integer -- do nothing, 
  class_y <- class(data_copy[[1]][,y])
  if(class_y %in% c('integer', 'numeric')) {
    class_y <- 'regr'
  } else {
    class_y <- 'classif'
    #Otherwise -- find the top 2 most frequent -- and make the others apart of the minority class
    #To make it 2 factor
    classes <- names(sort(table(data_copy[[i]][,y]), decreasing = TRUE))[1:2]
    
    #Eliminate everything that isn't in these classes
    for(i in 1:length(data_copy)) {
      data_copy[[i]][which(data_copy[[i]][,y] %in% classes == F),y] <- classes[2]
    }
  }
  
  ###############
  # Pre Processing #
  ###############
  
  #Near Zero variance
  nzv <- unique(do.call("c", lapply(1:length(data_copy), function(i) {
    nearZeroVar(data_copy[[i]][, -(match(y, colnames(data_copy[[i]])))], names = TRUE)
  })))
  
  #Remove the near zero variance columns
  if(length(nzv) > 0) {
    for(i in 1:length(data_copy)) {
      data_copy[[i]] <- data_copy[[i]][, !(colnames(data_copy[[i]]) %in% nzv)]
    }
  }
  
  #Now remove the remaining NA rows (if there are any)
  for(i in 1:length(data_copy)) {
    data_copy[[i]] <- data_copy[[i]] %>% 
      na.omit()
  }
  
  #Find highly correlated variables
  fc <- 
    unique(
      do.call("c", 
              lapply(1:length(data_copy), function(i) {
                #Correlation matrix
                mCor <- cor(data_copy[[i]][,-(match(y, colnames(data_copy[[i]])))])
                #Find correlation
                findCorrelation(mCor, cutoff = 0.975, exact = TRUE, names = TRUE)
              })))
  
  if(length(fc) > 0) {
    for(i in 1:length(data_copy)) {
      data_copy[[i]] <- data_copy[[i]][, !(colnames(data_copy[[i]]) %in% fc)]
    }
  }
  
  ###############
  # Create the Task #
  ###############
  
  #Create the task
  task <- lapply(1:length(data_copy), function(i) {
    if(class_y == 'regr') {
      makeRegrTask(
        data = data_copy[[i]] %>%
          dplyr::select(-one_of('id')),
        target = y
      )
    } else {
      makeClassifTask(
        data = data_copy[[i]] %>%
          dplyr::select(-one_of('id')),
        target = y, positive = pos_classes[k]
      )
    }
  })
  
  ###############
  # Set the Tuning Parameters #
  ###############
  
  #Set the hyper parameter search criteria
  rancontrol <- makeTuneControlRandom(maxit = 500L)
  
  #Set 3 Fold Cross Validation
  set_cv <- makeResampleDesc("CV", iters = 3L)
  
  #Define the number of times you want to iterate
  n_iterations <- 2
  
  #Define the maximum iterations
  n_max <- 300
  
  ###############
  # Build the Models #
  ###############
  
  rf_build <- rf_iterate(n_iterations, n_max, task, set_cv, rancontrol)
  xgb_build <- xgb_iterate(rf_build, n_iterations, task, set_cv, rancontrol)
  lm_build <- lm_iterate(rf_build, task)
  
  #Combine
  builds <- do.call("c", list(
    rf = rf_build,
    xgb = xgb_build,
    lm = lm_build
  ))
  
  #Initial reduction to the best 5 models
  best_builds <-
    builds %>%
    map(2) %>%
    map2_df(., names(.), ~ mutate(.x, ID = .y)) %>%
    melt(id.vars = c('ID', 'Set')) %>%
    spread(Set, value) %>%
    split(., .$variable) %>%
    map(., ~mutate(., DELTA = -1*(Test - Train)^2)) %>%
    map(., ~mutate_at(., c('Train', 'Test'), ~. * ifelse(class_y == 'regr', -1, 1))) %>%
    map(., ~pProc(., c('ID', 'variable', 'Train'), "range")) %>%
    invoke(rbind, .) %>%
    dplyr::select(-variable) %>%
    melt(id.vars = 'ID') %>%
    group_by(ID) %>%
    summarise(AVG = mean(value)) %>%
    arrange(desc(AVG)) %>%
    top_n(2, AVG) %>%
    .$ID %>%
    as.character
  
  ###############
  # Save the Models
  ###############
  
  saveRDS(builds[best_builds], paste0(path, target_variable, '.rds'))
}

